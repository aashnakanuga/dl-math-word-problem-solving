{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiGL_dropout_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc3TfwQ7PZhb",
        "colab_type": "code",
        "outputId": "3ffd97fc-c8d7-40be-b9ea-ff6a5c7e10f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x # enable TF 2.x in Colab\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x # enable TF 2.x in Colab`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnCgKUrrPjQs",
        "colab_type": "code",
        "outputId": "4776ff4d-5ed1-4330-d267-e3ba4df89946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0-rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u2YzspHkstP",
        "colab_type": "code",
        "outputId": "141360a7-8a03-4ffa-af4d-4d1c23e455f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount drive\n",
        "drive.mount('/gdrive')\n",
        "drive_root = '/gdrive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wNVhL5BxZZsh"
      },
      "source": [
        "### Creating the dataset of word problems\n",
        "\n",
        "*Please add the correct path to load the data file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZhTdlS9DZY3W",
        "colab": {}
      },
      "source": [
        "with open('data_final.pkl', 'rb') as f:\n",
        "  df = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dD1ge883ZYMh",
        "outputId": "2ea73005-0a29-48b7-8b33-9e46088608c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38144, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b3578c3d-3da7-4f62-c9ee-f473ce113025",
        "id": "mIc3Bv2KZW6-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Equation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>A painter needed to paint 12 rooms in a build...</td>\n",
              "      <td>X=(7.0*(12.0-5.0))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3897</th>\n",
              "      <td>Brenda had 253 raspberry. John gripped some ra...</td>\n",
              "      <td>X = 253 - 66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27626</th>\n",
              "      <td>Casey wants to share some Bread among 17 frien...</td>\n",
              "      <td>X = 39 * 17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32530</th>\n",
              "      <td>Liza had 34 Press. Thomas furnished him some m...</td>\n",
              "      <td>X = 64 - 34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16266</th>\n",
              "      <td>George wants to distribute 125 mangos among 25...</td>\n",
              "      <td>X = 125 / 25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Question            Equation\n",
              "963     A painter needed to paint 12 rooms in a build...  X=(7.0*(12.0-5.0))\n",
              "3897   Brenda had 253 raspberry. John gripped some ra...        X = 253 - 66\n",
              "27626  Casey wants to share some Bread among 17 frien...         X = 39 * 17\n",
              "32530  Liza had 34 Press. Thomas furnished him some m...         X = 64 - 34\n",
              "16266  George wants to distribute 125 mangos among 25...        X = 125 / 25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRqrupoGHK4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_eqn(eqn):\n",
        "  '''\n",
        "  Add a space between every character in the equation string.\n",
        "  Eg: 'x = 23 + 88' becomes 'x =  2 3 + 8 8'\n",
        "  '''\n",
        "  elements = list(eqn)\n",
        "  return ' '.join(elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52CngavAg1PM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_exps = list(df['Question'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BP-L86thNKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_exps = list(df['Equation'].apply(lambda x: convert_eqn(x)).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTRQklLBp-YC",
        "colab_type": "code",
        "outputId": "b605d2ae-5547-4399-b5fb-8ceb91f768fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Input: Word Problem\n",
        "input_exps[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' A painter needed to paint 12 rooms in a building. Each room takes 7 hours to paint. If he already painted 5 rooms, how much longer will he take to paint the rest? ',\n",
              " 'Brenda had 253 raspberry. John gripped some raspberry. Now Brenda has 66  raspberry. How many did John grippeds?',\n",
              " 'Casey wants to share some Bread among 17 friends.If each friend get 39 Bread, then how many Bread john would have?',\n",
              " 'Liza had 34 Press. Thomas furnished him some more. Now Liza has 64 Press. How many did Thomas furnish him?',\n",
              " 'George wants to distribute 125 mangos among 25 friends. How many would each friend acquire?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9DL8tD-6iP9",
        "colab_type": "code",
        "outputId": "d1147d81-29c3-46c6-d08c-0846aa532ddc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Target: Equation\n",
        "target_exps[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X = ( 7 . 0 * ( 1 2 . 0 - 5 . 0 ) )',\n",
              " 'X   =   2 5 3   -   6 6',\n",
              " 'X   =   3 9   *   1 7',\n",
              " 'X   =   6 4   -   3 4',\n",
              " 'X   =   1 2 5   /   2 5']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_MAlVH39Mlh",
        "colab_type": "code",
        "outputId": "a92ae4fd-176f-48c7-f08a-3c2c1f6f57be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(pd.Series(input_exps)), len(pd.Series(input_exps).unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38144, 38144)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3-dcn8YMe3",
        "colab_type": "code",
        "outputId": "fae4d8c8-0df2-4a7d-c290-e8b36b743a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(pd.Series(target_exps)), len(pd.Series(target_exps).unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38144, 23603)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0vtG7rdZdy",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing and Tokenizing the Input and Target exps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yx7HUtFYZri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_input(sentence):\n",
        "  '''\n",
        "  For the word problem, convert everything to lowercase, add spaces around all\n",
        "  punctuations and digits, and remove any extra spaces. \n",
        "  '''\n",
        "  sentence = sentence.lower().strip()\n",
        "  sentence = re.sub(r\"([?.!,â€™])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r\"([0-9])\", r\" \\1 \", sentence)\n",
        "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "  sentence = sentence.rstrip().strip()\n",
        "  return '<start> ' + sentence + ' <end>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfyWFYbo-Fkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_target(sentence):\n",
        "  '''\n",
        "  For the equation, convert it to lowercase and remove extra spaces\n",
        "  '''\n",
        "  sentence = sentence.lower().strip()\n",
        "  return '<start> ' + sentence + ' <end>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEAS9242ZUT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed_input_exps = list(map(preprocess_input, input_exps))\n",
        "preprocessed_target_exps = list(map(preprocess_target, target_exps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy-bGm81a8yl",
        "colab_type": "code",
        "outputId": "732c4c9f-ba4d-40cd-9ca8-ec0b87b47cff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "preprocessed_input_exps[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> a painter needed to paint 1 2 rooms in a building . each room takes 7 hours to paint . if he already painted 5 rooms , how much longer will he take to paint the rest ? <end>',\n",
              " '<start> brenda had 2 5 3 raspberry . john gripped some raspberry . now brenda has 6 6 raspberry . how many did john grippeds ? <end>',\n",
              " '<start> casey wants to share some bread among 1 7 friends . if each friend get 3 9 bread , then how many bread john would have ? <end>',\n",
              " '<start> liza had 3 4 press . thomas furnished him some more . now liza has 6 4 press . how many did thomas furnish him ? <end>',\n",
              " '<start> george wants to distribute 1 2 5 mangos among 2 5 friends . how many would each friend acquire ? <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g15oK28mbA89",
        "colab_type": "code",
        "outputId": "94deb926-dfa2-4fce-fa72-cb7c80e9ce04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "preprocessed_target_exps[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> x = ( 7 . 0 * ( 1 2 . 0 - 5 . 0 ) ) <end>',\n",
              " '<start> x   =   2 5 3   -   6 6 <end>',\n",
              " '<start> x   =   3 9   *   1 7 <end>',\n",
              " '<start> x   =   6 4   -   3 4 <end>',\n",
              " '<start> x   =   1 2 5   /   2 5 <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhkEBsfbJaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  '''\n",
        "  Tokenize the given list of strings and return the tokenized output\n",
        "  along with the fitted tokenizer.\n",
        "  '''\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL_eLZZAbsHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(preprocessed_input_exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI2k73bDceJ_",
        "colab_type": "code",
        "outputId": "fdd005fd-4c21-4ae3-bba2-e52f2a7eef67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(inp_lang_tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5889"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciyWH2a_b4gx",
        "colab_type": "code",
        "outputId": "be424842-579c-4946-9a68-f98a4ae7f83e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "input_tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   3,  106, 2456, ...,    0,    0,    0],\n",
              "       [   3,  282,    8, ...,    0,    0,    0],\n",
              "       [   3,  813,   45, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [   3,  382,   14, ...,    0,    0,    0],\n",
              "       [   3,  169,    8, ...,    0,    0,    0],\n",
              "       [   3,  406,    8, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVIuwYu9b7fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tensor, targ_lang_tokenizer = tokenize(preprocessed_target_exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bpeONmscRzr",
        "colab_type": "code",
        "outputId": "92ab5f8d-f310-4560-cb4a-ffe13608e558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(targ_lang_tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkFWp965cHbt",
        "colab_type": "code",
        "outputId": "28232ee7-b4de-4885-f22e-7e79700a9655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "target_tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 4, 1, ..., 0, 0, 0],\n",
              "       [2, 4, 1, ..., 0, 0, 0],\n",
              "       [2, 4, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [2, 4, 1, ..., 0, 0, 0],\n",
              "       [2, 4, 1, ..., 0, 0, 0],\n",
              "       [2, 4, 1, ..., 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKtDz1pdV11",
        "colab_type": "text"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWn3alPwcIMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating training and validation sets\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
        "                                                                                                target_tensor,\n",
        "                                                                                                test_size=0.05,\n",
        "                                                                                                random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkvMMb7NUG1l",
        "colab_type": "code",
        "outputId": "2a3a43a3-cdaf-461a-96b3-aa07cb963da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(input_tensor_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8VnUqe9UE9P",
        "colab_type": "code",
        "outputId": "022dd3bc-1ad3-4e05-f985-104ef7553339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(input_tensor_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFxzn930dDNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 32\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "dropout = 0.5\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpWhdlGQeQzB",
        "colab_type": "code",
        "outputId": "16a60232-c599-4284-ca01-effdcdc499c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_inp_size, vocab_tar_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5890, 48)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnLEanaeNOX",
        "colab_type": "code",
        "outputId": "ade4d6c2-9e43-458a-c246-38800cbd00ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 124]), TensorShape([64, 66]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuneDtDufIAe",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe6XY6NGewke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    \n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    # Bidirectional GRU Unit\n",
        "    self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = dropout))\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state1, state2 = self.gru(x, initial_state = hidden)\n",
        "    state = [state1,state2]\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)),tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pporVET9K89d",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtrkdaXIkTaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    \n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # Get attention_weights\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PC3mEUCsK6gj",
        "colab": {}
      },
      "source": [
        "attention_layer = BahdanauAttention(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-DyZg30oC9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    #LSTM Unit\n",
        "    self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    \n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the LSTM\n",
        "    output, state, cell_state = self.lstm(x)\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nnsOvkeBK37t",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9-Wv1eHoCNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # Apply a mask to paddings (0)\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWpq1TQftedU",
        "colab_type": "text"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxmuOCEuyrj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -r /gdrive/My\\ Drive/ADL\\ Project/checkpoints/training_checkpoints/aashna_bidir_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMDyLmwNJEr7",
        "colab_type": "code",
        "outputId": "cc1961ab-3eec-4d28-cf7a-82214ad42d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "checkpoint_dir = os.path.join(drive_root, \"ADL Project/checkpoints\")\n",
        "checkpoint_dir = os.path.join(checkpoint_dir, \"training_checkpoints/aashna_bidir_new\")\n",
        "\n",
        "print(\"Checkpoints directory is\", checkpoint_dir)\n",
        "if os.path.exists(checkpoint_dir):\n",
        "  print(\"Checkpoints folder already exists\")\n",
        "else:\n",
        "  print(\"Creating a checkpoints directory\")\n",
        "  os.makedirs(checkpoint_dir)\n",
        "\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoints directory is /gdrive/My Drive/ADL Project/checkpoints/training_checkpoints/aashna_bidir_new\n",
            "Creating a checkpoints directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFpsWcR-uVIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh8zrJl0uWoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if latest:\n",
        "  epoch_num = int(latest.split('/')[-1].split('-')[-1])\n",
        "  checkpoint.restore(latest)\n",
        "else:\n",
        "  epoch_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVHRFQuswBET",
        "colab_type": "code",
        "outputId": "e8e5dccd-2ab2-428a-b821-06ffadb7993a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epoch_num"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVFK-PEhKWcL",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZFzAVCKLfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    # print(enc_output.shape)\n",
        "    # print(len(enc_hidden))\n",
        "    # print(enc_hidden[0].shape)\n",
        "    dec_hidden = enc_hidden[0]\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input, to ensure proper training\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67qb5rLOKLXz",
        "colab_type": "code",
        "outputId": "f3a3c357-515a-48bc-bdd4-167deeb8fc4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(epoch_num, EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print('Saved epoch: {} at {}'.format(epoch+1, checkpoint_dir))\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.5086\n",
            "Epoch 1 Batch 100 Loss 0.2943\n",
            "Epoch 1 Batch 200 Loss 0.2160\n",
            "Epoch 1 Batch 300 Loss 0.2370\n",
            "Epoch 1 Batch 400 Loss 0.2115\n",
            "Epoch 1 Batch 500 Loss 0.2505\n",
            "Saved epoch: 1 at /gdrive/My Drive/ADL Project/checkpoints/training_checkpoints/aashna_bidir_new\n",
            "Epoch 1 Loss 0.2379\n",
            "Time taken for 1 epoch 390.9043769836426 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.1775\n",
            "Epoch 2 Batch 100 Loss 0.1959\n",
            "Epoch 2 Batch 200 Loss 0.1591\n",
            "Epoch 2 Batch 300 Loss 0.1599\n",
            "Epoch 2 Batch 400 Loss 0.1521\n",
            "Epoch 2 Batch 500 Loss 0.1344\n",
            "Saved epoch: 2 at /gdrive/My Drive/ADL Project/checkpoints/training_checkpoints/aashna_bidir_new\n",
            "Epoch 2 Loss 0.1610\n",
            "Time taken for 1 epoch 326.494660615921 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1497\n",
            "Epoch 3 Batch 100 Loss 0.1319\n",
            "Epoch 3 Batch 200 Loss 0.1441\n",
            "Epoch 3 Batch 300 Loss 0.1420\n",
            "Epoch 3 Batch 400 Loss 0.1098\n",
            "Epoch 3 Batch 500 Loss 0.1159\n",
            "Saved epoch: 3 at /gdrive/My Drive/ADL Project/checkpoints/training_checkpoints/aashna_bidir_new\n",
            "Epoch 3 Loss 0.1293\n",
            "Time taken for 1 epoch 326.1851215362549 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1036\n",
            "Epoch 4 Batch 100 Loss 0.0952\n",
            "Epoch 4 Batch 200 Loss 0.1001\n",
            "Epoch 4 Batch 300 Loss 0.1045\n",
            "Epoch 4 Batch 400 Loss 0.0831\n",
            "Epoch 4 Batch 500 Loss 0.0718\n",
            "Saved epoch: 4 at /gdrive/My Drive/ADL Project/checkpoints/training_checkpoints/aashna_bidir_new\n",
            "Epoch 4 Loss 0.0962\n",
            "Time taken for 1 epoch 324.4792175292969 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0812\n",
            "Epoch 5 Batch 100 Loss 0.0638\n",
            "Epoch 5 Batch 200 Loss 0.0891\n",
            "Epoch 5 Batch 300 Loss 0.0796\n",
            "Epoch 5 Batch 400 Loss 0.0981\n",
            "Epoch 5 Batch 500 Loss 0.0774\n",
            "Saved epoch: 5 at /gdrive/My Drive/ADL Project/checkpoints/training_checkpoints/aashna_bidir_new\n",
            "Epoch 5 Loss 0.0916\n",
            "Time taken for 1 epoch 324.4754981994629 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyexdqi0M8hH",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZyvWqNNNKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTJBsVz_NMPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn3ZLnj3JEpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    # Preprocess the input\n",
        "    sentence = preprocess_input(sentence)\n",
        "\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden[0]\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrQPiXzJEmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lejO34wRJEjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fExKUNvrJEg0",
        "colab_type": "code",
        "outputId": "2760a722-8edc-469a-9e78-b1710596e868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa8fb1d10f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOmRe3rePI90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_accuracy(inputs):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = ''\n",
        "    for i in range(len(inputs.numpy()[0])):\n",
        "      if inputs.numpy()[0][i] != 0: #padding token\n",
        "        sentence += inp_lang_tokenizer.index_word[inputs.numpy()[0][i]] + ' '\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "    result_seq = ''\n",
        "    \n",
        "    hidden = [tf.zeros((1, units)),tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    \n",
        "    dec_hidden = enc_hidden[0]\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "    \n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "        result_seq += str(predicted_id) +' '\n",
        "        \n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result_seq, result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result_seq, result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjHwLDy8TTYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "dataset_val = dataset_val.batch(1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZZyZW6LoCKm",
        "colab_type": "code",
        "outputId": "d1ed3e05-8d54-4147-f339-4f257cea8eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "acc_cnt = 0\n",
        "\n",
        "a = 0\n",
        "for (inp_val_batch, target_val_batch) in iter(dataset_val):\n",
        "  a += 1\n",
        "  if a % 50 == 0:\n",
        "    print(a)\n",
        "    print(\"Accuracy count: \",acc_cnt)\n",
        "    print('------------------')\n",
        "\n",
        "  target_sentence = ''\n",
        "  for i in target_val_batch.numpy()[0]:\n",
        "    if i!= 0:\n",
        "      target_sentence += (targ_lang_tokenizer.index_word[i] + ' ')\n",
        "  target_sentence = target_sentence.split('<start> ')[1]\n",
        "  # print('True:{}'.format(target_sentence))\n",
        "  y_true.append([target_sentence.split(' ')])\n",
        "\n",
        "  res_seq, res, sent, att = evaluate_accuracy(inp_val_batch)\n",
        "  y_pred.append(res.split(' '))\n",
        "  \n",
        "  if target_sentence == res:\n",
        "    acc_cnt += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "Accuracy count:  7\n",
            "------------------\n",
            "100\n",
            "Accuracy count:  21\n",
            "------------------\n",
            "150\n",
            "Accuracy count:  36\n",
            "------------------\n",
            "200\n",
            "Accuracy count:  47\n",
            "------------------\n",
            "250\n",
            "Accuracy count:  64\n",
            "------------------\n",
            "300\n",
            "Accuracy count:  81\n",
            "------------------\n",
            "350\n",
            "Accuracy count:  90\n",
            "------------------\n",
            "400\n",
            "Accuracy count:  108\n",
            "------------------\n",
            "450\n",
            "Accuracy count:  121\n",
            "------------------\n",
            "500\n",
            "Accuracy count:  136\n",
            "------------------\n",
            "550\n",
            "Accuracy count:  149\n",
            "------------------\n",
            "600\n",
            "Accuracy count:  165\n",
            "------------------\n",
            "650\n",
            "Accuracy count:  176\n",
            "------------------\n",
            "700\n",
            "Accuracy count:  192\n",
            "------------------\n",
            "750\n",
            "Accuracy count:  209\n",
            "------------------\n",
            "800\n",
            "Accuracy count:  221\n",
            "------------------\n",
            "850\n",
            "Accuracy count:  229\n",
            "------------------\n",
            "900\n",
            "Accuracy count:  242\n",
            "------------------\n",
            "950\n",
            "Accuracy count:  248\n",
            "------------------\n",
            "1000\n",
            "Accuracy count:  259\n",
            "------------------\n",
            "1050\n",
            "Accuracy count:  272\n",
            "------------------\n",
            "1100\n",
            "Accuracy count:  287\n",
            "------------------\n",
            "1150\n",
            "Accuracy count:  301\n",
            "------------------\n",
            "1200\n",
            "Accuracy count:  317\n",
            "------------------\n",
            "1250\n",
            "Accuracy count:  326\n",
            "------------------\n",
            "1300\n",
            "Accuracy count:  333\n",
            "------------------\n",
            "1350\n",
            "Accuracy count:  348\n",
            "------------------\n",
            "1400\n",
            "Accuracy count:  357\n",
            "------------------\n",
            "1450\n",
            "Accuracy count:  370\n",
            "------------------\n",
            "1500\n",
            "Accuracy count:  385\n",
            "------------------\n",
            "1550\n",
            "Accuracy count:  400\n",
            "------------------\n",
            "1600\n",
            "Accuracy count:  415\n",
            "------------------\n",
            "1650\n",
            "Accuracy count:  426\n",
            "------------------\n",
            "1700\n",
            "Accuracy count:  437\n",
            "------------------\n",
            "1750\n",
            "Accuracy count:  449\n",
            "------------------\n",
            "1800\n",
            "Accuracy count:  461\n",
            "------------------\n",
            "1850\n",
            "Accuracy count:  473\n",
            "------------------\n",
            "1900\n",
            "Accuracy count:  490\n",
            "------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daGaIvQW7riI",
        "colab_type": "code",
        "outputId": "11641c74-9306-44f5-d152-6355fece320d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Corpus BLEU score of the model: ', corpus_bleu(y_true, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus BLEU score of the model:  0.44778402085810237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcswXaKS5ssh",
        "colab_type": "code",
        "outputId": "c535a2d0-d8d9-46a8-ecbc-2ead8330d8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy of the model: ', acc_cnt/len(input_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model:  0.25681341719077566\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c90clFbl1WpE",
        "colab_type": "text"
      },
      "source": [
        "#### Translation and Attention Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow23VYqqatBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_str = ' '.join([inp_lang_tokenizer.index_word[i] for i in input_tensor_val[0] if i != 0][1:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOlMl19G0yCN",
        "colab_type": "code",
        "outputId": "88a1f0f7-10a0-4cda-bfea-d29c7a0cfff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "check_str"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ronnie had some blackberry . he divide each blackberry into 1 3 slices . if total 1 8 9 blackberry slices ronnie make , then how many blackberry ronnie had ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiveRxOVqIHw",
        "colab_type": "code",
        "outputId": "f1d7e782-9bfa-4ff8-ccd5-ae33d35bd492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "translate(check_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> ronnie had some blackberry . he divide each blackberry into 1 3 slices . if total 1 8 9 blackberry slices ronnie make , then how many blackberry ronnie had ? <end>\n",
            "Predicted translation: x = 1 9 8 / 1 3 <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAEMCAYAAACiMkDAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt3EzdK81GrY",
        "colab_type": "code",
        "outputId": "19acea8b-5d34-4a4d-c297-fe3719cd6326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "translate(\"Jerry had 135 pens. John took 19 from him. How many does Jerry have left?\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> jerry had 1 3 5 pens . john took 1 9 from him . how many does jerry have left ? <end>\n",
            "Predicted translation: x = 1 3 5 - 1 9 <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAFDCAYAAABcPPh5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJElqAZM6SZKkFvj/pb8pg8SYI6sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAdJzJr1CaXy",
        "colab_type": "text"
      },
      "source": [
        "### Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp_vTUIWCaLK",
        "colab_type": "text"
      },
      "source": [
        "This model does not overfit as the baseline did with the old dataset. It gives reasonable scores for the dataset. However, we can see from the attention plots that even though a lot of translations are correct or very close, the word it is supposed to give to attention to while translating is not correct.\n",
        "\n",
        "This is why we also decided to experiment with Transformer models as well to see if it will give better results.\n",
        "\n",
        "Sources:\n",
        "1. https://web.stanford.edu/class/cs224n/reports/custom/15843468.pdf\n",
        "2. https://www.tensorflow.org/tutorials/text/nmt_with_attention#top_of_page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfDUfkhEFUrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}