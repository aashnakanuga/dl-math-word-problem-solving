{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc3TfwQ7PZhb",
        "colab_type": "code",
        "outputId": "37342e16-fac8-42d4-cc51-a22d5c581946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x # enable TF 2.x in Colab\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `2.x # enable TF 2.x in Colab`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnCgKUrrPjQs",
        "colab_type": "code",
        "outputId": "f7e0a5fd-677a-45aa-d233-41025adb9cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0-rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u2YzspHkstP",
        "colab_type": "code",
        "outputId": "6eca6610-78b1-4695-a362-dbbbe67e28f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount drive\n",
        "drive.mount('/gdrive')\n",
        "drive_root = '/gdrive/My Drive/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xx2ORAXdeJk",
        "colab_type": "text"
      },
      "source": [
        "### Creating a custom dataset of expressions(target) and math word problems(input)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd32CVu1ZRx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Function to convert integers to English words\n",
        "def numberToWords(num):\n",
        "  \"\"\"\n",
        "  :type num: int\n",
        "  :rtype: str\n",
        "  \"\"\"\n",
        "  def one(num):\n",
        "      switcher = {\n",
        "          1: 'One',\n",
        "          2: 'Two',\n",
        "          3: 'Three',\n",
        "          4: 'Four',\n",
        "          5: 'Five',\n",
        "          6: 'Six',\n",
        "          7: 'Seven',\n",
        "          8: 'Eight',\n",
        "          9: 'Nine'\n",
        "      }\n",
        "      return switcher.get(num)\n",
        "\n",
        "  def two_less_20(num):\n",
        "      switcher = {\n",
        "          10: 'Ten',\n",
        "          11: 'Eleven',\n",
        "          12: 'Twelve',\n",
        "          13: 'Thirteen',\n",
        "          14: 'Fourteen',\n",
        "          15: 'Fifteen',\n",
        "          16: 'Sixteen',\n",
        "          17: 'Seventeen',\n",
        "          18: 'Eighteen',\n",
        "          19: 'Nineteen'\n",
        "      }\n",
        "      return switcher.get(num)\n",
        "  \n",
        "  def ten(num):\n",
        "      switcher = {\n",
        "          2: 'Twenty',\n",
        "          3: 'Thirty',\n",
        "          4: 'Forty',\n",
        "          5: 'Fifty',\n",
        "          6: 'Sixty',\n",
        "          7: 'Seventy',\n",
        "          8: 'Eighty',\n",
        "          9: 'Ninety'\n",
        "      }\n",
        "      return switcher.get(num)\n",
        "  \n",
        "\n",
        "  def two(num):\n",
        "      if not num:\n",
        "          return ''\n",
        "      elif num < 10:\n",
        "          return one(num)\n",
        "      elif num < 20:\n",
        "          return two_less_20(num)\n",
        "      else:\n",
        "          tenner = num // 10\n",
        "          rest = num - tenner * 10\n",
        "          return ten(tenner) + ' ' + one(rest) if rest else ten(tenner)\n",
        "  \n",
        "  def three(num):\n",
        "      hundred = num // 100\n",
        "      rest = num - hundred * 100\n",
        "      if hundred and rest:\n",
        "          return one(hundred) + ' Hundred ' + two(rest) \n",
        "      elif not hundred and rest: \n",
        "          return two(rest)\n",
        "      elif hundred and not rest:\n",
        "          return one(hundred) + ' Hundred'\n",
        "  \n",
        "  billion = num // 1000000000\n",
        "  million = (num - billion * 1000000000) // 1000000\n",
        "  thousand = (num - billion * 1000000000 - million * 1000000) // 1000\n",
        "  rest = num - billion * 1000000000 - million * 1000000 - thousand * 1000\n",
        "  \n",
        "  if not num:\n",
        "      return 'Zero'\n",
        "  \n",
        "  result = ''\n",
        "  if billion:        \n",
        "      result = three(billion) + ' Billion'\n",
        "  if million:\n",
        "      result += ' ' if result else ''    \n",
        "      result += three(million) + ' Million'\n",
        "  if thousand:\n",
        "      result += ' ' if result else ''\n",
        "      result += three(thousand) + ' Thousand'\n",
        "  if rest:\n",
        "      result += ' ' if result else ''\n",
        "      result += three(rest)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHfgKeREP37f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## creating a simple dataset with random number of operations(one, two or three operations),\n",
        "## random number (uptil 50000) and random operations\n",
        "def create_simple_dataset(dataset_len = 1000):\n",
        "  ops_dict = {'+':['plus','add'],'-':['minus','subtract'],'*':['multiply','into'],'/':['divide','by']}\n",
        "\n",
        "\n",
        "  nums_dict = {}\n",
        "  \n",
        "  for i in range(50000):\n",
        "    nums_dict[i] = numberToWords(i)\n",
        "  \n",
        "\n",
        "  num_ops = [2,3,4]\n",
        "  nums_dict_keys_list = list(nums_dict.keys())\n",
        "  ops_dict_keys_list = list(ops_dict.keys())\n",
        "\n",
        "  all_input_exps = []\n",
        "  all_target_exps = []\n",
        "  for _ in range(dataset_len):\n",
        "    n_o = random.choice(num_ops)\n",
        "    \n",
        "    nums = []\n",
        "    num_vals = []\n",
        "    for i in range(n_o+1):\n",
        "      nums.append(random.choice(nums_dict_keys_list))\n",
        "      num_vals.append(nums_dict[nums[i]])\n",
        "\n",
        "    ops = []\n",
        "    ops_vals = []\n",
        "    for i in range(n_o):\n",
        "      ops.append(random.choice(ops_dict_keys_list))\n",
        "      ops_vals.append(random.choice(ops_dict[ops[i]]))\n",
        "\n",
        "    target_exp = ' '.join(list(str(nums[0])))\n",
        "    inp_exp = num_vals[0]\n",
        "    for i in range(n_o):\n",
        "      target_exp = target_exp + ' ' + ops[i] + ' ' + ' '.join(list(str(nums[i+1])))\n",
        "      inp_exp = inp_exp + ' ' + ops_vals[i] + ' ' + num_vals[i+1]\n",
        "\n",
        "    all_input_exps.append(inp_exp)\n",
        "    all_target_exps.append(target_exp)\n",
        "\n",
        "  return all_input_exps,all_target_exps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAFyxA0TQgY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_exps, target_exps = create_simple_dataset(2000000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTRQklLBp-YC",
        "colab_type": "code",
        "outputId": "dabe40e3-3e5b-46be-a82d-60e99de4ad98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "input_exps[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Thirty Five Thousand Six Hundred Twenty divide Forty Eight Thousand Nine Hundred Eight minus Forty Nine Thousand Two Hundred Nineteen plus Forty Eight Thousand Seven Hundred Thirty Four',\n",
              " 'Forty Eight Thousand Nine Hundred Fifty Six into Sixteen Thousand Seven Hundred Thirty Four minus Thirteen Thousand Ninety One add Twenty Nine Thousand One Hundred Five multiply Thirty Thousand One Hundred Thirty Three',\n",
              " 'Twenty Six Thousand Two Hundred Twenty One into Forty Six Thousand Three Hundred Fifty Five by Thirty Four Thousand Seven Hundred Forty Eight add Thirteen Thousand Eighty One add Three Thousand Three Hundred Ninety Two',\n",
              " 'Twenty Eight Thousand Four Hundred Fifty Five by Twenty Thousand Four Hundred Nine subtract Nine Thousand Eight Hundred Forty Five minus Thirty Nine Thousand Two Hundred Sixty Seven',\n",
              " 'Forty Three Thousand Five Hundred Eighty subtract Forty Six Thousand Two Hundred Fifty Nine divide Two Thousand Five Hundred Sixty Four']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9DL8tD-6iP9",
        "colab_type": "code",
        "outputId": "f15c256b-fd0d-4754-aabd-897b5336f158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "target_exps[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3 5 6 2 0 / 4 8 9 0 8 - 4 9 2 1 9 + 4 8 7 3 4',\n",
              " '4 8 9 5 6 * 1 6 7 3 4 - 1 3 0 9 1 + 2 9 1 0 5 * 3 0 1 3 3',\n",
              " '2 6 2 2 1 * 4 6 3 5 5 / 3 4 7 4 8 + 1 3 0 8 1 + 3 3 9 2',\n",
              " '2 8 4 5 5 / 2 0 4 0 9 - 9 8 4 5 - 3 9 2 6 7',\n",
              " '4 3 5 8 0 - 4 6 2 5 9 / 2 5 6 4']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3-dcn8YMe3",
        "colab_type": "code",
        "outputId": "9e9ef366-a7fd-4614-d49c-f3a500b4b3f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(pd.Series(target_exps)), len(pd.Series(target_exps).unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000000, 2000000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vVlLI-hkKHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.DataFrame({'Input':input_exps, 'Target':target_exps})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO3UwKJqkPXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_pickle('data-baseline.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0vtG7rdZdy",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing and Tokenizing the Input and Target exps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtRjHNtUlKG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data-baseline.pkl', 'rb') as f:\n",
        "  data = pickle.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ4_tIumlVWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_exps = list(data.Input.values)\n",
        "target_exps = list(data.Target.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv7NjOZz62uH",
        "colab_type": "code",
        "outputId": "824d5553-89f8-4d7c-b1c8-37e8bf93f727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "input_exps[0], target_exps[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Thirty Five Thousand Six Hundred Twenty divide Forty Eight Thousand Nine Hundred Eight minus Forty Nine Thousand Two Hundred Nineteen plus Forty Eight Thousand Seven Hundred Thirty Four',\n",
              " '3 5 6 2 0 / 4 8 9 0 8 - 4 9 2 1 9 + 4 8 7 3 4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yx7HUtFYZri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## right now, only adding start and end token\n",
        "## Can later preprocess more: add spaces before and after punctuations, replace unimp tokens, etc.\n",
        "def preprocess_strings(sentence):\n",
        "  sentence = sentence.lower().strip()\n",
        "  return '<start> ' + sentence + ' <end>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEAS9242ZUT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed_input_exps = list(map(preprocess_strings, input_exps))\n",
        "preprocessed_target_exps = list(map(preprocess_strings, target_exps))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKhkEBsfbJaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL_eLZZAbsHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(preprocessed_input_exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI2k73bDceJ_",
        "colab_type": "code",
        "outputId": "30075035-e76a-49e6-93f0-59237ad9c344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(inp_lang_tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciyWH2a_b4gx",
        "colab_type": "code",
        "outputId": "45482b02-7024-4ffb-b8db-128ede51b9c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "input_tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15,  5,  7, ...,  0,  0,  0],\n",
              "       [15,  3,  9, ...,  0,  0,  0],\n",
              "       [15,  4, 12, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [15, 32,  1, ...,  0,  0,  0],\n",
              "       [15,  4,  8, ...,  0,  0,  0],\n",
              "       [15, 13,  1, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVIuwYu9b7fH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_tensor, targ_lang_tokenizer = tokenize(preprocessed_target_exps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bpeONmscRzr",
        "colab_type": "code",
        "outputId": "36ddc2af-eea3-4ec7-fc77-7005505c64e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(targ_lang_tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkFWp965cHbt",
        "colab_type": "code",
        "outputId": "f7af9fea-33cc-46fd-8bf8-8ac5454ffefb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "target_tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11,  3,  9, ...,  0,  0,  0],\n",
              "       [11,  1,  7, ...,  3,  3, 12],\n",
              "       [11,  2,  8, ...,  2, 12,  0],\n",
              "       ...,\n",
              "       [11,  4,  7, ...,  0,  0,  0],\n",
              "       [11,  2,  1, ...,  0,  0,  0],\n",
              "       [11,  6,  5, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWKtDz1pdV11",
        "colab_type": "text"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWn3alPwcIMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating training and validation sets\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.01, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkvMMb7NUG1l",
        "colab_type": "code",
        "outputId": "295409f8-bfdb-4fea-ee9a-7553642baced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(input_tensor_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1980000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8VnUqe9UE9P",
        "colab_type": "code",
        "outputId": "8f17dbe0-ab1c-40b8-d24b-6f3b8edd09f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(input_tensor_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFxzn930dDNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 512\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 64\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpWhdlGQeQzB",
        "colab_type": "code",
        "outputId": "68d98078-318f-4ffe-ded7-6890c231c817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_inp_size, vocab_tar_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BnLEanaeNOX",
        "colab_type": "code",
        "outputId": "76324f22-4b70-4bcd-da42-a5f3c810b66b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([512, 41]), TensorShape([512, 31]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuneDtDufIAe",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe6XY6NGewke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buE9VlnSjBYS",
        "colab_type": "code",
        "outputId": "c3081bd7-618d-478f-c75f-48d089b605f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (512, 41, 256)\n",
            "Encoder Hidden state shape: (batch size, units) (512, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtrkdaXIkTaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DhdGtYxoC_y",
        "colab_type": "code",
        "outputId": "93dcc34f-8321-463f-a0b8-b676f4e48d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(100)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (512, 256)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (512, 41, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-DyZg30oC9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUO4H1W3oC6n",
        "colab_type": "code",
        "outputId": "8bbc49a8-6644-43cf-e467-c59ed3440fad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (512, 17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3OrLXr3oCSY",
        "colab_type": "code",
        "outputId": "391fcabc-d492-46d5-ec16-cbe148b20b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_hidden.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([512, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9-Wv1eHoCNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWpq1TQftedU",
        "colab_type": "text"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxmuOCEuyrj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -r /gdrive/My\\ Drive/checkpoints/ADL\\ Project/training_checkpoints/aashna_baseline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMDyLmwNJEr7",
        "colab_type": "code",
        "outputId": "5008dae7-e425-4da0-c1b6-35354d7f6b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "checkpoint_dir = os.path.join(drive_root, \"checkpoints\")\n",
        "checkpoint_dir = os.path.join(checkpoint_dir, \"ADL Project/training_checkpoints/aashna_baseline\")\n",
        "\n",
        "print(\"Checkpoints directory is\", checkpoint_dir)\n",
        "if os.path.exists(checkpoint_dir):\n",
        "  print(\"Checkpoints folder already exists\")\n",
        "else:\n",
        "  print(\"Creating a checkpoints directory\")\n",
        "  os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checkpoints directory is /gdrive/My Drive/checkpoints/ADL Project/training_checkpoints/aashna_baseline\n",
            "Checkpoints folder already exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFpsWcR-uVIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "busbpqF8wT02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh8zrJl0uWoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if latest:\n",
        "  epoch_num = int(latest.split('/')[-1].split('-')[-1])\n",
        "  checkpoint.restore(latest)\n",
        "else:\n",
        "  epoch_num = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVHRFQuswBET",
        "colab_type": "code",
        "outputId": "4dacd808-5aec-42fa-b943-22c209b1caa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "epoch_num"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVFK-PEhKWcL",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3ZFzAVCKLfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67qb5rLOKLXz",
        "colab_type": "code",
        "outputId": "95c21108-4be6-4bf8-bcac-eea8b6dd3c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(epoch_num, EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "  print('Saved epoch: {}'.format(epoch+1))\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1338\n",
            "Epoch 1 Batch 100 Loss 1.8552\n",
            "Epoch 1 Batch 200 Loss 1.6436\n",
            "Epoch 1 Batch 300 Loss 1.6049\n",
            "Epoch 1 Batch 400 Loss 1.6153\n",
            "Epoch 1 Batch 500 Loss 1.5431\n",
            "Epoch 1 Batch 600 Loss 1.5223\n",
            "Epoch 1 Batch 700 Loss 1.5490\n",
            "Epoch 1 Batch 800 Loss 1.5062\n",
            "Epoch 1 Batch 900 Loss 1.4740\n",
            "Epoch 1 Batch 1000 Loss 1.4543\n",
            "Epoch 1 Batch 1100 Loss 1.3986\n",
            "Epoch 1 Batch 1200 Loss 1.3051\n",
            "Epoch 1 Batch 1300 Loss 1.0260\n",
            "Epoch 1 Batch 1400 Loss 0.9643\n",
            "Epoch 1 Batch 1500 Loss 1.0915\n",
            "Epoch 1 Batch 1600 Loss 0.4079\n",
            "Epoch 1 Batch 1700 Loss 0.2810\n",
            "Epoch 1 Batch 1800 Loss 0.2225\n",
            "Epoch 1 Batch 1900 Loss 0.1507\n",
            "Epoch 1 Batch 2000 Loss 0.0709\n",
            "Epoch 1 Batch 2100 Loss 1.1790\n",
            "Epoch 1 Batch 2200 Loss 0.4887\n",
            "Epoch 1 Batch 2300 Loss 1.8735\n",
            "Epoch 1 Batch 2400 Loss 1.4612\n",
            "Epoch 1 Batch 2500 Loss 1.4240\n",
            "Epoch 1 Batch 2600 Loss 1.4220\n",
            "Epoch 1 Batch 2700 Loss 1.3512\n",
            "Epoch 1 Batch 2800 Loss 1.3432\n",
            "Epoch 1 Batch 2900 Loss 1.3149\n",
            "Epoch 1 Batch 3000 Loss 1.2295\n",
            "Epoch 1 Batch 3100 Loss 1.3098\n",
            "Epoch 1 Batch 3200 Loss 1.1933\n",
            "Epoch 1 Batch 3300 Loss 1.1791\n",
            "Epoch 1 Batch 3400 Loss 1.3744\n",
            "Epoch 1 Batch 3500 Loss 1.1806\n",
            "Epoch 1 Batch 3600 Loss 1.1476\n",
            "Epoch 1 Batch 3700 Loss 1.1047\n",
            "Epoch 1 Batch 3800 Loss 1.1040\n",
            "Saved epoch: 1\n",
            "Epoch 1 Loss 1.2191\n",
            "Time taken for 1 epoch 549.5104856491089 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.0977\n",
            "Epoch 2 Batch 100 Loss 1.0690\n",
            "Epoch 2 Batch 200 Loss 1.0479\n",
            "Epoch 2 Batch 300 Loss 1.0744\n",
            "Epoch 2 Batch 400 Loss 1.1075\n",
            "Epoch 2 Batch 500 Loss 1.0445\n",
            "Epoch 2 Batch 600 Loss 1.0606\n",
            "Epoch 2 Batch 700 Loss 1.3910\n",
            "Epoch 2 Batch 800 Loss 0.9565\n",
            "Epoch 2 Batch 900 Loss 0.7743\n",
            "Epoch 2 Batch 1000 Loss 0.5367\n",
            "Epoch 2 Batch 1100 Loss 0.5777\n",
            "Epoch 2 Batch 1200 Loss 0.2405\n",
            "Epoch 2 Batch 1300 Loss 0.2123\n",
            "Epoch 2 Batch 1400 Loss 0.2261\n",
            "Epoch 2 Batch 1500 Loss 0.0432\n",
            "Epoch 2 Batch 1600 Loss 0.0331\n",
            "Epoch 2 Batch 1700 Loss 0.0183\n",
            "Epoch 2 Batch 1800 Loss 0.0236\n",
            "Epoch 2 Batch 1900 Loss 0.0152\n",
            "Epoch 2 Batch 2000 Loss 0.0116\n",
            "Epoch 2 Batch 2100 Loss 0.0125\n",
            "Epoch 2 Batch 2200 Loss 0.0110\n",
            "Epoch 2 Batch 2300 Loss 0.0133\n",
            "Epoch 2 Batch 2400 Loss 0.0108\n",
            "Epoch 2 Batch 2500 Loss 0.0097\n",
            "Epoch 2 Batch 2600 Loss 0.2718\n",
            "Epoch 2 Batch 2700 Loss 0.0165\n",
            "Epoch 2 Batch 2800 Loss 0.0136\n",
            "Epoch 2 Batch 2900 Loss 0.0154\n",
            "Epoch 2 Batch 3000 Loss 0.0098\n",
            "Epoch 2 Batch 3100 Loss 0.0094\n",
            "Epoch 2 Batch 3200 Loss 0.0111\n",
            "Epoch 2 Batch 3300 Loss 0.0068\n",
            "Epoch 2 Batch 3400 Loss 0.0087\n",
            "Epoch 2 Batch 3500 Loss 0.0093\n",
            "Epoch 2 Batch 3600 Loss 0.0074\n",
            "Epoch 2 Batch 3700 Loss 0.0067\n",
            "Epoch 2 Batch 3800 Loss 0.0081\n",
            "Saved epoch: 2\n",
            "Epoch 2 Loss 0.3153\n",
            "Time taken for 1 epoch 520.5215473175049 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.0108\n",
            "Epoch 3 Batch 100 Loss 0.0073\n",
            "Epoch 3 Batch 200 Loss 0.0713\n",
            "Epoch 3 Batch 300 Loss 0.0138\n",
            "Epoch 3 Batch 400 Loss 0.0103\n",
            "Epoch 3 Batch 500 Loss 0.0075\n",
            "Epoch 3 Batch 600 Loss 0.0085\n",
            "Epoch 3 Batch 700 Loss 0.0059\n",
            "Epoch 3 Batch 800 Loss 0.0080\n",
            "Epoch 3 Batch 900 Loss 0.0048\n",
            "Epoch 3 Batch 1000 Loss 0.0053\n",
            "Epoch 3 Batch 1100 Loss 0.0049\n",
            "Epoch 3 Batch 1200 Loss 0.0036\n",
            "Epoch 3 Batch 1300 Loss 0.0044\n",
            "Epoch 3 Batch 1400 Loss 0.0039\n",
            "Epoch 3 Batch 1500 Loss 0.0032\n",
            "Epoch 3 Batch 1600 Loss 0.0033\n",
            "Epoch 3 Batch 1700 Loss 0.0040\n",
            "Epoch 3 Batch 1800 Loss 0.0019\n",
            "Epoch 3 Batch 1900 Loss 0.0033\n",
            "Epoch 3 Batch 2000 Loss 0.0030\n",
            "Epoch 3 Batch 2100 Loss 0.0031\n",
            "Epoch 3 Batch 2200 Loss 0.0118\n",
            "Epoch 3 Batch 2300 Loss 0.3696\n",
            "Epoch 3 Batch 2400 Loss 0.0071\n",
            "Epoch 3 Batch 2500 Loss 0.0054\n",
            "Epoch 3 Batch 2600 Loss 0.0070\n",
            "Epoch 3 Batch 2700 Loss 0.0061\n",
            "Epoch 3 Batch 2800 Loss 0.0042\n",
            "Epoch 3 Batch 2900 Loss 0.0039\n",
            "Epoch 3 Batch 3000 Loss 0.0027\n",
            "Epoch 3 Batch 3100 Loss 0.0024\n",
            "Epoch 3 Batch 3200 Loss 0.0016\n",
            "Epoch 3 Batch 3300 Loss 0.0017\n",
            "Epoch 3 Batch 3400 Loss 0.0018\n",
            "Epoch 3 Batch 3500 Loss 0.0006\n",
            "Epoch 3 Batch 3600 Loss 0.0004\n",
            "Epoch 3 Batch 3700 Loss 0.0005\n",
            "Epoch 3 Batch 3800 Loss 0.0003\n",
            "Saved epoch: 3\n",
            "Epoch 3 Loss 0.0104\n",
            "Time taken for 1 epoch 520.3780183792114 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0003\n",
            "Epoch 4 Batch 100 Loss 0.0003\n",
            "Epoch 4 Batch 200 Loss 0.0006\n",
            "Epoch 4 Batch 300 Loss 0.0487\n",
            "Epoch 4 Batch 400 Loss 0.0064\n",
            "Epoch 4 Batch 500 Loss 0.0057\n",
            "Epoch 4 Batch 600 Loss 0.0075\n",
            "Epoch 4 Batch 700 Loss 0.0050\n",
            "Epoch 4 Batch 800 Loss 0.0063\n",
            "Epoch 4 Batch 900 Loss 0.0045\n",
            "Epoch 4 Batch 1000 Loss 0.0045\n",
            "Epoch 4 Batch 1100 Loss 0.0049\n",
            "Epoch 4 Batch 1200 Loss 0.0060\n",
            "Epoch 4 Batch 1300 Loss 0.0061\n",
            "Epoch 4 Batch 1400 Loss 0.0046\n",
            "Epoch 4 Batch 1500 Loss 0.0016\n",
            "Epoch 4 Batch 1600 Loss 0.0009\n",
            "Epoch 4 Batch 1700 Loss 0.0007\n",
            "Epoch 4 Batch 1800 Loss 0.0004\n",
            "Epoch 4 Batch 1900 Loss 0.0006\n",
            "Epoch 4 Batch 2000 Loss 0.0002\n",
            "Epoch 4 Batch 2100 Loss 0.0002\n",
            "Epoch 4 Batch 2200 Loss 0.0003\n",
            "Epoch 4 Batch 2300 Loss 0.0002\n",
            "Epoch 4 Batch 2400 Loss 0.0008\n",
            "Epoch 4 Batch 2500 Loss 0.0008\n",
            "Epoch 4 Batch 2600 Loss 0.0001\n",
            "Epoch 4 Batch 2700 Loss 0.0001\n",
            "Epoch 4 Batch 2800 Loss 0.0004\n",
            "Epoch 4 Batch 2900 Loss 0.0004\n",
            "Epoch 4 Batch 3000 Loss 0.0001\n",
            "Epoch 4 Batch 3100 Loss 0.0001\n",
            "Epoch 4 Batch 3200 Loss 0.0074\n",
            "Epoch 4 Batch 3300 Loss 0.0002\n",
            "Epoch 4 Batch 3400 Loss 0.0001\n",
            "Epoch 4 Batch 3500 Loss 0.0006\n",
            "Epoch 4 Batch 3600 Loss 0.0003\n",
            "Epoch 4 Batch 3700 Loss 0.0002\n",
            "Epoch 4 Batch 3800 Loss 0.0001\n",
            "Saved epoch: 4\n",
            "Epoch 4 Loss 0.0046\n",
            "Time taken for 1 epoch 520.692554473877 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0004\n",
            "Epoch 5 Batch 100 Loss 0.0001\n",
            "Epoch 5 Batch 200 Loss 0.0001\n",
            "Epoch 5 Batch 300 Loss 0.0002\n",
            "Epoch 5 Batch 400 Loss 0.0006\n",
            "Epoch 5 Batch 500 Loss 0.0000\n",
            "Epoch 5 Batch 600 Loss 0.0000\n",
            "Epoch 5 Batch 700 Loss 0.0002\n",
            "Epoch 5 Batch 800 Loss 0.0001\n",
            "Epoch 5 Batch 900 Loss 0.0001\n",
            "Epoch 5 Batch 1000 Loss 0.0001\n",
            "Epoch 5 Batch 1100 Loss 0.0000\n",
            "Epoch 5 Batch 1200 Loss 0.0000\n",
            "Epoch 5 Batch 1300 Loss 0.0000\n",
            "Epoch 5 Batch 1400 Loss 0.0000\n",
            "Epoch 5 Batch 1500 Loss 0.0000\n",
            "Epoch 5 Batch 1600 Loss 0.0000\n",
            "Epoch 5 Batch 1700 Loss 0.0000\n",
            "Epoch 5 Batch 1800 Loss 0.0000\n",
            "Epoch 5 Batch 1900 Loss 0.0000\n",
            "Epoch 5 Batch 2000 Loss 0.0000\n",
            "Epoch 5 Batch 2100 Loss 0.0000\n",
            "Epoch 5 Batch 2200 Loss 0.0000\n",
            "Epoch 5 Batch 2300 Loss 1.3604\n",
            "Epoch 5 Batch 2400 Loss 1.1388\n",
            "Epoch 5 Batch 2500 Loss 0.9041\n",
            "Epoch 5 Batch 2600 Loss 1.1194\n",
            "Epoch 5 Batch 2700 Loss 1.2000\n",
            "Epoch 5 Batch 2800 Loss 0.0963\n",
            "Epoch 5 Batch 2900 Loss 0.0408\n",
            "Epoch 5 Batch 3000 Loss 0.0872\n",
            "Epoch 5 Batch 3100 Loss 0.0231\n",
            "Epoch 5 Batch 3200 Loss 0.0167\n",
            "Epoch 5 Batch 3300 Loss 0.0108\n",
            "Epoch 5 Batch 3400 Loss 0.0083\n",
            "Epoch 5 Batch 3500 Loss 0.0060\n",
            "Epoch 5 Batch 3600 Loss 0.0075\n",
            "Epoch 5 Batch 3700 Loss 0.0661\n",
            "Epoch 5 Batch 3800 Loss 0.0139\n",
            "Saved epoch: 5\n",
            "Epoch 5 Loss 0.1236\n",
            "Time taken for 1 epoch 520.2507231235504 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eyexdqi0M8hH",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZyvWqNNNKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTJBsVz_NMPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn3ZLnj3JEpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_strings(sentence)\n",
        "\n",
        "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umrQPiXzJEmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lejO34wRJEjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fExKUNvrJEg0",
        "colab_type": "code",
        "outputId": "9dea6646-b91e-4e0c-d561-b80c089aa886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f32dcd8d2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOmRe3rePI90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_accuracy(inputs):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = ''\n",
        "    for i in range(len(inputs.numpy()[0])):\n",
        "      if inputs.numpy()[0][i] != 0:\n",
        "        sentence += inp_lang_tokenizer.index_word[inputs.numpy()[0][i]] + ' '\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "    result_seq = ''\n",
        "    \n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    \n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "    \n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        \n",
        "        result_seq += str(predicted_id) +' '\n",
        "        \n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result_seq, result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result_seq, result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjHwLDy8TTYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
        "dataset_val = dataset_val.batch(1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZZyZW6LoCKm",
        "colab_type": "code",
        "outputId": "95c2fb56-01b0-447f-9fa1-83ea691817b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "acc_cnt = 0\n",
        "\n",
        "a = 0\n",
        "for (inp_val_batch, target_val_batch) in iter(dataset_val):\n",
        "  a += 1\n",
        "  if a % 500 == 0:\n",
        "    print(a)\n",
        "    print(\"Accuracy count: \",acc_cnt)\n",
        "    print('------------------')\n",
        "  target_sentence = ''\n",
        "  for i in target_val_batch.numpy()[0]:\n",
        "    if i!= 0:\n",
        "      target_sentence += (targ_lang_tokenizer.index_word[i] + ' ')\n",
        "  target_sentence = target_sentence.split('<start> ')[1]\n",
        "  \n",
        "  y_true.append([target_sentence.split(' ')])\n",
        "\n",
        "  res_seq, res, sent, att = evaluate_accuracy(inp_val_batch)\n",
        "  y_pred.append(res.split(' '))\n",
        "  \n",
        "  if target_sentence == res:\n",
        "    acc_cnt += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "Accuracy count:  481\n",
            "------------------\n",
            "1000\n",
            "Accuracy count:  967\n",
            "------------------\n",
            "1500\n",
            "Accuracy count:  1454\n",
            "------------------\n",
            "2000\n",
            "Accuracy count:  1935\n",
            "------------------\n",
            "2500\n",
            "Accuracy count:  2411\n",
            "------------------\n",
            "3000\n",
            "Accuracy count:  2901\n",
            "------------------\n",
            "3500\n",
            "Accuracy count:  3376\n",
            "------------------\n",
            "4000\n",
            "Accuracy count:  3858\n",
            "------------------\n",
            "4500\n",
            "Accuracy count:  4345\n",
            "------------------\n",
            "5000\n",
            "Accuracy count:  4829\n",
            "------------------\n",
            "5500\n",
            "Accuracy count:  5310\n",
            "------------------\n",
            "6000\n",
            "Accuracy count:  5789\n",
            "------------------\n",
            "6500\n",
            "Accuracy count:  6268\n",
            "------------------\n",
            "7000\n",
            "Accuracy count:  6753\n",
            "------------------\n",
            "7500\n",
            "Accuracy count:  7234\n",
            "------------------\n",
            "8000\n",
            "Accuracy count:  7720\n",
            "------------------\n",
            "8500\n",
            "Accuracy count:  8202\n",
            "------------------\n",
            "9000\n",
            "Accuracy count:  8684\n",
            "------------------\n",
            "9500\n",
            "Accuracy count:  9161\n",
            "------------------\n",
            "10000\n",
            "Accuracy count:  9641\n",
            "------------------\n",
            "10500\n",
            "Accuracy count:  10120\n",
            "------------------\n",
            "11000\n",
            "Accuracy count:  10602\n",
            "------------------\n",
            "11500\n",
            "Accuracy count:  11081\n",
            "------------------\n",
            "12000\n",
            "Accuracy count:  11561\n",
            "------------------\n",
            "12500\n",
            "Accuracy count:  12048\n",
            "------------------\n",
            "13000\n",
            "Accuracy count:  12534\n",
            "------------------\n",
            "13500\n",
            "Accuracy count:  13015\n",
            "------------------\n",
            "14000\n",
            "Accuracy count:  13497\n",
            "------------------\n",
            "14500\n",
            "Accuracy count:  13982\n",
            "------------------\n",
            "15000\n",
            "Accuracy count:  14468\n",
            "------------------\n",
            "15500\n",
            "Accuracy count:  14952\n",
            "------------------\n",
            "16000\n",
            "Accuracy count:  15435\n",
            "------------------\n",
            "16500\n",
            "Accuracy count:  15917\n",
            "------------------\n",
            "17000\n",
            "Accuracy count:  16410\n",
            "------------------\n",
            "17500\n",
            "Accuracy count:  16897\n",
            "------------------\n",
            "18000\n",
            "Accuracy count:  17378\n",
            "------------------\n",
            "18500\n",
            "Accuracy count:  17864\n",
            "------------------\n",
            "19000\n",
            "Accuracy count:  18351\n",
            "------------------\n",
            "19500\n",
            "Accuracy count:  18830\n",
            "------------------\n",
            "20000\n",
            "Accuracy count:  19311\n",
            "------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIaALZGCBTWN",
        "colab_type": "code",
        "outputId": "f858de6e-d1c0-4b93-bbb3-c2452a7b57f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Corpus BLEU score of the model: ', corpus_bleu(y_true, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus BLEU score of the model:  0.9951898934618143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcswXaKS5ssh",
        "colab_type": "code",
        "outputId": "6ee0b521-4d75-4725-a536-03afcc94e78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy of the model: ',acc_cnt/len(input_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model:  0.9656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c90clFbl1WpE",
        "colab_type": "text"
      },
      "source": [
        "#### Translation and Attention Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZWk14NP_fh4",
        "colab_type": "code",
        "outputId": "539f2894-4ce7-44eb-db9d-765e530aee63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "check_str = 'One Thousand Three Hundred Nine subtract Fourteen Thousand Three Hundred Nine'\n",
        "check_str in input_exps"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpagV8m3_hXZ",
        "colab_type": "code",
        "outputId": "fb3ec2b7-97b4-40b3-c33e-8f27f5797d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "translate(check_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> one thousand three hundred nine subtract fourteen thousand three hundred nine <end>\n",
            "Predicted translation: 1 3 0 9 - 1 4 3 0 9 <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAJdCAYAAABUCIEPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbs4L3vWTtMr",
        "colab_type": "code",
        "outputId": "b987d690-5ea0-4aa9-dbc1-9c64bccf99d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        }
      },
      "source": [
        "translate(\"thirty plus one hundred twenty five\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> thirty plus one hundred twenty five <end>\n",
            "Predicted translation: 3 0 0 5 5 <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAI8CAYAAACXoF3wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDgBgAEIdAMAAhDoAgAEIdQAAA/j/AXnSBjsRfMATAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azzion5Ns8kz",
        "colab_type": "code",
        "outputId": "0bd03ca6-1333-4e12-feeb-cc80fde9c571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "translate('One plus Two')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> one plus two <end>\n",
            "Predicted translation: 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHUAAAJwCAYAAAC3Vi4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAUAUlEQVR4nO2de7BdZXnGf+8JCZcEvECMYIsoF0EK\ncglCTAspVpliqyOl2nIvHUBqhZY6TC29YDsDA6WCjiDGsWgJozCZ0VKHe8FCAbGAeAFHLkZAVJpA\naEJCSEje/vGtk2w2J+esdfY+e20ent/MnrPP2nu/5zv72d9a3/qeZ307MhOjxUjbDTD9x6IKYlEF\nsaiCWFRBLKogFlUQiyqIRRXEogrSmqgRsXtE3BoR+7TVBlXa7KknAguAk1tsgyTRxoR+RATwM+Bm\n4PeBnTJz/cAbIkpbPXUBsC1wBvAScGRL7ZCkLVFPBBZn5mrg69Xvpk8MfPcbETOBXwIfyMw7ImI/\n4G5gx8x8bqCNEaWNnvoHwLLMvAMgMx8AHgH+qIW2DB0RMTMiToiI1022RhuiHg8s6tq2CDhp8E0Z\nSj4CXEF5nybFQHe/EfHrwBJgr8x8pGP7r1FGw+/MzIcH1qAhJCJuA+YAqzNz7qRqOKM0PETELsDD\nwLuB7wAHZOZDTesMfPcbETtX56ljPjbo9gwZxwN3VOOM65jkWUEbx9QlwOzujRGxffXYa5kTgCur\n+1cBx26uA4xHG6IGMNY+fxawZsBtGRoi4j3AjsDiatN/ANsAv9O01hZ9bNe4RMTnqrsJnB8Rqzse\nnkY5jjwwqPYMIScC/56ZzwNk5tqIuIZyVnBzk0IDExUYdWMC2AtY2/HYWuB+4KIBtmdoiIgtKacy\nf9z10CLgxoiYNSp2rXoDPqUJ4Brg5MxcObA/PORExA6U+e9Fmbmh67HjgFsy81e16w1Y1GmU4+a7\nJjNUN/UY6ECpstceB2YM8u++1mhjQv9EyrHjuMxcNtA/PmRExBLGPhN4BZn59rp1BzlQGuWTwNuA\npyLi58Cqzgczc98W2tQWn++4Pws4C/guxbUCmEc5K/iXJkXbEHXxxE95bZCZG8WKiK8AF2TmeZ3P\niYhPAXs3qeu53yEhIlZQ5nof7dq+G3B/Zm5Xt5YjosPDKkrMp5sFwOoxtm+Wge9+I2IGcA5lsLQz\nML3z8cycNug2DQkXA5dGxFyKQwNwCGWm6dxGlTJzoDfgAop3ehrlE3gW8Dngf4HTBt2eYbpRZpXu\nBJ6tbncCH2lap41TmiXA6Zl5Q0SsBPbLzMci4nTgvZl59EAbJEgbo985wOhs0vPA66v7N1B68cCI\niNkAmbm0+n0f4KPAg5n5tUG2patdr6drvJOZz9Z9fRsDpSeAnar7jwJHVPfnAS8MuC3XUMLko/Ov\ntwMfBi6PiL8aZEMi4q0RcX1EvAA8Ayytbsuqn7Vpo6d+A3gvZTDwWeBrEXEK8Bbgn5sUqoTYFXgg\nM1+cRFv2ZdOg5Gjg0cw8KCI+VLWl0Ul/j1xB2Wv9KfALas40jUXr56kRcTAwH3g4M79V8zXbAl+m\nCJHA7pn504i4HPhVZp5bs85qYM/MfCIiFgPfz8x/qgJyD2fm1jXrzAE+Dryzas9DwGWZ+XSd11c1\nngcOycwf1X3N5mgjo3RoRGzcQ2TmPZn5GeCGiDi0ZpkLKD37AF6+y/4WZfdZl0eAoyoR3w/cVG2f\nA9QKlkfEfMph5JiqLWuAY4FHImJeg7YsAbZs8PzN08KwfT3wpjG2bw+sr1nj58BB1f2VwNur+7sC\nKxu05SjgxapNN3VsPwe4rmaNu4GFwEjHtpFq210N2nI45UO1W8/vcQuibgBmj7F9D2BFzRqrOoTs\nFHU/4LmG7ZkD7N8lysGU3XKd178AvGOM7XsCLzRox0pKAmQ95fx9Reetyf80yIzStdXdBBZFROfA\nZhrwG8BdNcv9D/BB4JKOmlAmNOrWKC8sx72nu7bd06DE/1Fcp590bX8bNXfhFX/e4LnjMsjR7zPV\nzwCW8/Jj4Vrgv4Ev1az1N5Tszt6U/+Gs6v67gbrH5c4w3Jhk5hk1ynwd+HJEnM2mD9R8ynG/9rlu\nZn617nMnYmCiZuafAETEz4CLMnPV+K8Yt9ZdVaTyk8BjlFOk+4F5mfnDBqW6lyaYTtltTgO+V7PG\n2ZQP6r+y6f1cB3wB+OsGbRkdRR9PGRv8XWYuqwZiv8jM2pnoNqYJRwCyClhFxJuB3wMeysxGu86p\nICK2opwu3ZGZlzd43TYUMQAey3LtbZO/eyDwn5RR8N6UY/pPI+JcYI/MPKZ2sRYGStcDZ1b3Z1FG\nssspn+4TGtbaiTI4OqDz1oc27g08WfO57wG26MPfvA34dMegaXTwNw94vFGtFkRdCuxT3T+BcqI+\nnRJa/kHNGvsDD1JGihu6brVOiyaofxiwvOZz11Qi3EQ51k9KZMood6wR/S7Amia12pgmnMWmUeH7\ngW9k5rqIuBW4tGaNhcCTwCn0MKUWEWd1b6Jc+nAs5QKlOryBMjA6DPhd4O+BdRFxN3BbZp5fs84L\nVa1u9qTYkvVpoaf+hHLV+ExKr11Qbd8PWFqzxirKcabXtizpuj1GmQs+D9h2kjV3pQya1tJgr0H5\noF5LmVVaSTkl2gX4PnBxoza0IOpplOPncsq1MyPV9jOAW2vW+A5w6KDbvpm2vIlibn8B+HH1gbsN\n+AfgsAZ1tqOc1q2gHFaeoqxcczsws0mb2lpH6UBKlOXmrK4RiYgPUGaD7qzx+sMpvelvgR9SPiQb\nyQbeY69ExAbKHueLwC3APTk5x2i03uGUAd8IJXB2S+MagxS1Wpxi36wW8eh6bD7ltGZ5jTqd15t0\n/gMBZI6Tc5powqGTrDH5EBGLKBMerwPuoPTSb1MEqfXm9ut92fiaAYu6LWW5nSM6e2REvIsSYn5L\n1kjtR8Rh4z2emf81zmtvq9nczMzDaz6XiNiVkvxbQBF5O+D2zPxQjdf25X3Z+LpB734j4irg+cw8\nrWPbRZSBzwcb1OnZw+yqNwsgG1wy2PX6EeAgitvy2xRxMzNr2Wn9el+AVgZKR1CScjOq30copyVH\nNagxnzKgeJRyOf2V1f0VlKnCJu35C0rEZn11exL4S6oPfI3Xn005/VlBsfHuAs6v/s/aA5x+vC8b\na7Ug6ghlZHdU9fv7KDmc6Q1q9MvDvJByznwOpYcdXt1fDlxYs8YvqzovE5FyfN95kO9La6JWDb4A\n+GZ1/9+ASxu+vl8e5rPA0WNsPxp4pmaNDfRo+vfrfRm9tTGjNNrg+6oldj5McVma0C8PE+AHm9nW\nJOrTr4VJen1fgBaDZxFxL6XH7ZCZezV87SXAH1KOZ90e5tWZ2T39N16dyMwzu7ZfDEzLcU5pOk6N\nPk5JAo61MMnazJxfpy0ddSf9vozSVk+F8qm8hHIMa0q/PMwtgWMi4gg2RUUPprg/V3We044h8FQt\nTNLL+1Ia1GJPfSPwCeCL2WCRiq4avXqYPZ+zRsQVFCtxRZO/PU6ben9f2hLVTB2+PlUQiypI66JG\nxKnDUEOpLa2LCvTjTejLG9mnOq23ZRhENX1mSke/M2LL3IqZ4z5nHS8yfYLrgvbYd/wzlaXPrGf2\n9uMvFfHIQ9uO+zjA2g1rmDGy1bjPyZfG/06kOv9PHerUWcnyZZn5irWTp3TyYStmcnBMaqbrZdx4\nY+8rxh65b+/tAFi/7JmJnzQgbsnFj4+13btfQSyqIBZVkFqiVld/XxsRT0VERsRJU9wu0wN1e+os\n4EfAmQx+BRXTkFqj38y8juoyhGq1SzPE9P2UppreOhVgK7bpd3lTg74PlDJzYWbOzcy5/TgJN83x\n6FcQiyqIRRWk1kCpuiRht+rXEWDnKF87/WxmPjFVjTOTo25PnUtZreR7wNbAp6v7/zhF7TI9UPc8\n9duUKKR5FeBjqiBthrlrc8RO+/WhyvD4oIz06bsfNuPXu6cKYlEFsaiCWFRBGosaEZ+qjPLPT/xs\n0waNRI2IQyi22lgX6pohobao1Vo/VwEnU9ZEMENKk566EFicmXWv6TQtUXdC/xTKhP5xNZ7r5EPL\nTChqRLyDsg7gb2bmuomen5kLKb2a7eKNvqK5Ber01HnADsCDERvn9KcBh0bExyhrB016gUXTf+qI\n+k3g3q5tV1C+fek8Xr6AhRkCJhQ1M5+ja22iiFhFMch7/l4y0388oyTIpKy3zFzQ53aYPuKeKsir\nwiQfJqbNfsWF24158qTd+9AS4MKrx9zsniqIRRXEogpiUQWxqII08VP/LCKWRMSaiLgvIn5rKhtm\nJk/dNR8+CnyWMte7P2U17OurZcHNkFG3p54FfCUzv5SZP87MT1C+5eH0qWuamSwTihoRM4ADKd8P\n2slNlO8KNUNGnZ66A8U/7f5mpqeBN3c/OSJOjYh7I+LeddhmbQOv+SBIHVGXUS7FmdO1fQ4wqYX7\nzdQyoaiZuRa4j/I1VZ28j03fCWOGiLouzWeAKyPiu8CdwMco391y+VQ1zEyeuleSXx0R21O+WXhH\nypJ2R2bmmOvNmnap7adm5mXAZVPYFtMnPPcriJMPTckNEz9nAp7fY8JMfE+4pwpiUQWxqIJYVEFs\nkgtik1wQm+SC2CQXxCa5IDbJBbFJLohNckFskgtik1wQm+SCeO5XEJvkDcnVvX/T6MzHpvehJZvH\nPVUQiyqIRRXEogpiUQVx8kEQJx8EcfJBECcfBHHyQRAnHwRx8kEQJx8EcfJBECcfBHHyQRDP/Qri\n5ENTNn012qSZ9WTvSwyMh3uqIBZVEIsqiEUVxCa5IDbJBbFJLohNckFskgtik1wQm+SC2CQXxCa5\nIDbJBbFJLojnfgWxSd6Ukd77wdbLXupDQzaPe6ogFlUQiyqIRRXEogri5IMgTj4I4uSDIE4+COLk\ngyBOPgji5IMgTj4I4uSDIE4+COLkgyCe+xXEyYeGxLTe+8EWq5x8MA2xqIJYVEEsqiA2yQWxSS6I\nTXJBbJILYpNcEJvkgtgkF8QmuSA2yQWxSS6ITXJBPPcriE3ypkTv/WBk7fo+NGSc+lNa3bSCRRXE\nogpiUQWxqII4+SCIkw+COPkgiJMPgjj5IIiTD4I4+SCIkw+COPkgiJMPgjj5IIjnfgVx8qEFYkNO\naX33VEEsqiAWVRCLKohFFcTJB0GcfBDEyQdB+p58sEnePn1PPtgkbx+PfgVx8kEQJx8EcfJBECcf\nBHHyQRCb5G2QNslNQyyqIBZVEIsqiEUVxCa5IDbJBbFJLoiXBxDEywMI4uUBBLFJLohNckFskgti\nk1wQm+SCeO5XECcf2mDD1JZ3TxXEogpiUQWxqIJYVEGcfBDEyQdBnHwQxMsDCOLlAQTx6FcQJx8E\ncfJBECcfBHHyQRAnHwTx6FcQiyqIRRXEogpiUQWxqILYJBfEJrkgNskF8fIAgnh5AEG8PIAgNskF\nsUkuiE1yQWySC2KTXBDP/QpiUQWxqIJYVEEsqiAWVRCLKoiTD4I4+SCIkw+CeHkAQbw8gCAe/Qri\n5IMgTj4I4uSDIE4+COLkgyAe/QriL0Zogcic0vruqYJYVEEsqiAWVRCb5ILYJBfEJrkgXh5AEC8P\nIIiXBxDEJrkgNskFsUkuiE1yQWySC+K5X0EsqiAWVRCLKohFFcSiCmJRBXHyQRAnHwRx8kEQLw8g\niJcHEMSjX0GcfBDEyQdBnHwQxMkHQZx8EMSjX0EsqiAWVRCLKohFFcSiCmKTXBCb5ILYJBfEywMI\n4uUBBPHyAILYJBfEJrkgNskFsUkuiE1yQTz3K4hFFcSiCmJRBbGoglhUQSyqIE4+COLkgyBOPgji\n5IMgTj4I4uSDIE4+COLkgyBOPgji5IMgTj4I4rlfQSyqIBZVEIsqiEUVxKIKYpNcEJvkgtgkF8Qm\nuSA2yQWxSS6ITXJBbJILYpNcEJvkgtgkF8Rzv4JYVEEsqiAWVRCLKohFFcSiCuLkgyBOPgji5IMg\nTj4I4uSDIE4+COLkgyBOPgji5IMgTj4I4uSDIJ77FcSiCmJRBbGoglhUQSyqIDbJBbFJLohNckFs\nkgtik1wQm+SC2CQXxCa5IDbJBbFJLohNckE89yuIRRXEogpiUQWxqIJYVEEsqiBOPgji5IMgTj4I\n4uSDIE4+COLkgyBOPgji5IMgTj4I4uSDIE4+COK5X0EsqiAWVRCLKohFFcSiCmKTXBCb5ILYJBfE\nJrkgNskFsUkuiE1yQWySC2KTXBCb5ILYJBfEc7+CWFRBLKogFlUQiyqIRRXEogri5IMgTj4I4uSD\nIE4+COLkgyBOPgji5IMgTj4I4uSDIE4+COLkgyCe+xXEogpiUQWxqIJYVEEsqiAWVRAnHwRx8kEQ\nJx8E6XvywSZ5+/Q9+WCTvH08+hXEyQdBnHwQxMkHQZx8EMTJB0E8+hXEogpiUQWxqIJYVEEsqiA2\nyQWxSS6ITXJBvDyAIF4eQBAvDyCITXJBbJILYpNcEJvkgtgkF8Rzv4JYVEEsqiAWVRCLKohFFcSi\nCuLkgyBOPgji5IMgXh5AEC8PIIhHv4I4+SCIkw+COPkgiJMPgjj5IIhHv4JYVEEsqiAWVRCLKohF\nFcQmuSA2yQWxSS6IlwcQxMsDCOLlAQSxSS6ITXJBbJILYpNcEJvkgnjuVxCLKohFFcSiCmJRBbGo\nglhUQZx8EMTJB0GcfBDEywMI4uUBBPHoVxAnHwRx8kEQJx8EcfJBECcfBPHoVxCLKohFFcSiCmJR\nBbGogtgkF8QmuSA2yQXx8gCCeHkAQbw8gCA2yQWxSS6ITXJBbJILYpNcEM/9CmJRBbGoglhUQSyq\nIBZVEIsqiJMPgjj5IIiTD4I4+SCIkw+COPkgiJMPgjj5IIiTD4I4+SCIkw+CeO5XkMjMqSsesRSY\naBe9A2WE3Qv9qPFqbMtbM3N298YpFbUOEXFvZs5tu4ZSW7z7FcSiCjIMoi4ckhr9qtN6W1o/ppr+\nMww91fQZiyqIRRXEogpiUQX5fyof9p5e1z9fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXvLCFUl5-i8",
        "colab_type": "text"
      },
      "source": [
        "### Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbmRpK395-Tr",
        "colab_type": "text"
      },
      "source": [
        "We can see that this baseline model on this simple dataset is clearly memorizing the data rather than learning the actual way to translate the words to an equation.\n",
        "The Corpus BLEU and Accuracy scores are very high, but the translation fails on simple equations like \"One plus Two\"\n",
        "\n",
        "Sources:\n",
        "1. https://www.tensorflow.org/tutorials/text/nmt_with_attention#top_of_page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd86HZPyFGz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}